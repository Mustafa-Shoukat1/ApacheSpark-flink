# Production Spark Configuration

# Application Properties
spark.app.name                 Production-Spark-App
spark.master                   spark://spark-master:7077

# Driver Configuration
spark.driver.memory            2g
spark.driver.cores             2
spark.driver.maxResultSize     1g
spark.driver.memoryFraction    0.8

# Executor Configuration
spark.executor.memory          2g
spark.executor.cores           2
spark.executor.instances       4
spark.executor.memoryFraction  0.8
spark.executor.heartbeatInterval 10s
spark.executor.logs.rolling.strategy time
spark.executor.logs.rolling.time.interval daily
spark.executor.logs.rolling.maxRetainedFiles 7

# Dynamic Allocation
spark.dynamicAllocation.enabled                true
spark.dynamicAllocation.minExecutors           2
spark.dynamicAllocation.maxExecutors           10
spark.dynamicAllocation.initialExecutors       4
spark.dynamicAllocation.executorIdleTimeout    60s
spark.dynamicAllocation.cachedExecutorIdleTimeout 300s
spark.dynamicAllocation.shuffleTracking.enabled true

# Shuffle Configuration
spark.shuffle.service.enabled  true
spark.shuffle.service.port     7337
spark.shuffle.manager          sort
spark.shuffle.compress         true
spark.shuffle.spill.compress   true
spark.shuffle.file.buffer      32k

# Memory Management
spark.storage.memoryFraction   0.6
spark.storage.safetyFraction   0.9
spark.storage.unrollFraction   0.2
spark.storage.replication.proactive true

# Serialization
spark.serializer               org.apache.spark.serializer.KryoSerializer
spark.kryo.unsafe              false
spark.kryo.registrationRequired false
spark.kryoserializer.buffer.max 512m

# Network Configuration
spark.network.timeout          120s
spark.rpc.askTimeout           120s
spark.rpc.lookupTimeout        120s
spark.sql.adaptive.enabled     true
spark.sql.adaptive.coalescePartitions.enabled true
spark.sql.adaptive.skewJoin.enabled true
spark.sql.adaptive.localShuffleReader.enabled true

# Checkpointing and Recovery
spark.sql.streaming.checkpointLocation /opt/spark-apps/checkpoints
spark.sql.recovery.checkpointDir /opt/spark-apps/recovery

# Delta Lake Configuration
spark.sql.extensions           io.delta.sql.DeltaSparkSessionExtension
spark.sql.catalog.spark_catalog org.apache.spark.sql.delta.catalog.DeltaCatalog
spark.databricks.delta.retentionDurationCheck.enabled false

# Kafka Configuration
spark.sql.kafka.consumer.cache.enabled true
spark.sql.kafka.consumer.cache.capacity 256

# S3 Configuration
spark.hadoop.fs.s3a.impl                    org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.fast.upload             true
spark.hadoop.fs.s3a.block.size              128M
spark.hadoop.fs.s3a.multipart.size          100M
spark.hadoop.fs.s3a.multipart.threshold     128M
spark.hadoop.fs.s3a.connection.maximum      20

# Metrics Configuration
spark.metrics.conf.driver.source.jvm.class  org.apache.spark.metrics.source.JvmSource
spark.metrics.conf.executor.source.jvm.class org.apache.spark.metrics.source.JvmSource
spark.metrics.conf.*.sink.prometheusServlet.class org.apache.spark.metrics.sink.PrometheusServlet
spark.metrics.conf.*.sink.prometheusServlet.path /metrics/prometheus

# Event Log Configuration
spark.eventLog.enabled         true
spark.eventLog.dir             /opt/spark-apps/logs/spark-events
spark.eventLog.compress        true
spark.eventLog.rolling.enabled true
spark.eventLog.rolling.maxFileSize 128m

# History Server Configuration
spark.history.fs.logDirectory  /opt/spark-apps/logs/spark-events
spark.history.ui.port          18080

# UI Configuration
spark.ui.enabled               true
spark.ui.port                  4040
spark.ui.retainedJobs          1000
spark.ui.retainedStages        1000
spark.ui.retainedTasks         100000
spark.ui.retainedExecutors     1000

# Security Configuration
spark.authenticate             false
spark.network.crypto.enabled   false
spark.io.encryption.enabled    false

# Optimization
spark.sql.files.maxPartitionBytes 128MB
spark.sql.files.openCostInBytes   4194304
spark.sql.broadcastTimeout        300
spark.sql.shuffle.partitions       200

# Python Configuration
spark.pyspark.python           python3
spark.pyspark.driver.python    python3